Title,html,Resume
data scientist,"<div><div><div><div><div>Summary</div></div><div><div><div><ul><li>7+ years of IT
Industry experience in in designing and developing data mining/analytics solutions,
data centric integration, developing and
maintaining Business analytics. </li><li>Adept in data query, data migration, data analysis, predictive
modeling, machine learning, data mining, and data visualization implementations with extensive use of SQL, Python, R, Java, and Unix Shell scripting with platform of Toad, Oracle developer, Jupyter Notebook, Pycharm, R-Studio, Tableau, Hadoop with Spark.</li><li>Experience in
predictive analytic procedures used in supervised learning (Classification,
Regression, Decision trees,
Random Forest, SVM, Neural Networks),
unsupervised learning (Clustering-k-Means and PCA), and reinforcement learning.</li><li>Solid theory background for machine learning, data mining, text mining, graph mining, statistics modeling, NLP and deep learning.</li><li>Expert in Natural
Language Processing like POS Tagging, Parsing, Named Entity Recognition,
Relationship extraction, Information       Retrieval, Sentimental
Classification, Machine Translation, etc.</li><li>Solid knowledge of deep learning
algorithms like CNN, RNN, LSTM, GRU, etc. for text mining and image processing.</li><li>Professional in writing complex SQL queries on Oracle, MS SQL server, Teradata and MySQL using a lot of subqueries, joins, aggregate functions, analytical functions, and temporary tables, etc.Worked on Big Data Analytics, Hadoop ecosystems (Hadoop, Sqoop, Hive, Pig, Mapreduce) and Spark for big data migration, cleaning, transformation, processing, query and analysis</li><li>Familiar with software lifecycle includes requirement collecting/documentation, development, and testing for Unit, Smoke , integration, system, non-functional testing regarding performance,
scalability, usability, enduration,
load, and volume testing, and regression
testing, multivariate testing (A/B testing) and system maintenance.</li><li>Work with business domain experts and application developers to identify data relevant analysis / mining and, develop new predictive / analytical modeling methods and/or tools in Financial (like loan and foreign exchange), Product, Customer, Sales domains, etc.</li><li>Experience in data
aggregation and reduction techniques of large data sets with high performance
and parallel computing for high performance
analytical projects.</li><li>Involved in
diagnosing and resolving predictive / analytical model performance issues,
monitoring analytical system performance

·      
and implementing
efficiency improvements.</li><li>Conversant with MS
SQL/ Oracle PLSQL and RDBMS. Contributed in data definitions for new database
file/table development and/or changes to
existing ones as needed for analysis and mining purpose.</li><li>Experienced Oracle
PL/SQL Developer for designing, developing, debugging , maintaining and
administrating database in Oracle RDBMS. Solid
experience in PL/SQL and SQL programming and performance tuning.</li><li> Familiarity with
Oracle data warehousing features such as star &amp; snowflake data modeling
schemas, materialized views, bitmap indexes, Index
Organized Tables, external tables etc., and OLTP system using B-tree index,
Hash Join, etc.</li><li>Experienced in
front-end developing using Java, Javascript, C++ and back-end developing using
C.</li><li>Professional in
integrating and maintaining code using version control tools PVCS, SVN,
CVS.</li><li>Solid experience and
knowledge in ETL and Data warehousing concepts.Data Processing experience in
designing, implementing transformation
processes using ETL tool. </li><li>Involved in all aspects of ETL- requirement gathering
with standard interfaces to be used by operational
sources, data cleaning, data loading strategies, ETL mappings designing,
documentation and ETL jobs performance testing.</li><li>
Using Unix
bash/corn shell scripting to do backend process operation, system resources

checking, job scheduling, batch data loading, performance tuning and reporting.
</li><li>Conversant with
Project Management deliverables and SDLC phases – Waterfall and Agile.</li><li> A self-starter, team
player, excellent communicator, prolific researcher.</li><li>Expert technical
documentation skills. Strong interpersonal and communication skills (both
written and oral); ability to communicate with
people in a wide variety of areas and at various levels from technical
specialists to senior management. </li></ul></div> </div></div></div></div></div><div class=""parentContainer"" id=""CONTAINER_PARENT_1""><div class=""left-box"" id=""CONTAINER_2""><div class=""section education section SECTION_EDUC firstsection multi-para"" data-section-cd=""EDUC"" id=""SECTION_EDUC0d4e307a-85d8-4703-bb12-3153bbb24ae4""><div class=""heading"">
<div class=""sectiontitle"" id=""SECTIONNAME_EDUC""><strong>Education</strong></div></div><div class=""paragraph PARAGRAPH_EDUC firstparagraph"" id=""PARAGRAPH_EDUC_65580a83-7b4b-486d-b598-a6febadd364b""><div class=""clearfix doc-item""> <div class=""singlecolumn""> <span class=""paddedline"" dependency=""SCHO""> <span class=""companyname txt-bold"" id=""FIELD_SCHO"">University of California</span> </span> <span class=""paddedline"" dependency=""SCIT|SSTA|GRYR|GRED|GRST|GRIP""> <span class=""joblocation jobcity"" id=""FIELD_SCIT"">Los Angeles</span><span dependency=""SCIT+SSTA"">, </span><span class=""joblocation jobstate"" id=""FIELD_SSTA"">CA</span> <span dependency=""GRYR|GRED|GRST|GRIP""><span class=""septr"" dependency=""SSTA|SCIT""></span></span> <span class=""xslt_static_change displayNoneThisField"">Expected in </span> <span format=""%m/%Y"" id=""FIELD_GRYR"">2016</span> <span class=""jobdates"" format=""%m/%Y"" id=""FIELD_GRST""></span><span class=""hide"" dependency=""GRST+GRED""> – </span><span class=""jobdates"" format=""%m/%Y"" id=""FIELD_GRED""></span> <span class=""hide"" dependency=""GRED|GRST""><span class=""hide"" dependency=""GRIP"">–</span></span> <span id=""FIELD_GRIP""></span> </span> <span class=""paddedline degreeGap"" dependency=""DGRE|STUY|GRPA|GRHN""> <span class=""degree txt-bold txtItl"" id=""FIELD_DGRE"">Master of Science</span><span dependency=""DGRE+STUY""><span class=""beforecolonspace""> </span><span class=""txt-bold"">:</span></span> <span class=""programline"" id=""FIELD_STUY"">Computer Science</span> <span dependency=""DGRE|STUY""><span class=""hide"" dependency=""GRHN""> - </span></span><span id=""FIELD_GRHN""></span> <span class=""xslt_static_change hide"" dependency=""GRPA"">GPA</span><span class=""hide"" dependency=""GRPA""><span class=""beforecolonspace""> </span><span>:</span> </span><span id=""FIELD_GRPA""></span> </span> <span id=""FIELD_FRFM""><div>GPA:3.78 </div><div>Courses: -Statistics Programming -Databases and
Knowledge Bases -Graphs and Network Flows
-Language and Thought -Current Topics in
Computer Theory(Machine Learning Algorithm) -Computer Science Classics(Basic
Data Science) -Data Mining and
Big Data Analytics -System Security

</div></span> </div> </div></div><div class=""paragraph PARAGRAPH_EDUC"" id=""PARAGRAPH_EDUC_ac7d7b2b-d6bc-4286-90dc-d2e7e2ac5846""><div class=""clearfix doc-item""> <div class=""singlecolumn""> <span class=""paddedline"" dependency=""SCHO""> <span class=""companyname txt-bold"" id=""FIELD_SCHO"">University of Bridgeport</span> </span> <span class=""paddedline"" dependency=""SCIT|SSTA|GRYR|GRED|GRST|GRIP""> <span class=""joblocation jobcity"" id=""FIELD_SCIT"">Bridgeport</span><span dependency=""SCIT+SSTA"">, </span><span class=""joblocation jobstate"" id=""FIELD_SSTA"">CT</span> <span dependency=""GRYR|GRED|GRST|GRIP""><span class=""septr"" dependency=""SSTA|SCIT""></span></span> <span class=""xslt_static_change displayNoneThisField"">Expected in </span> <span format=""%m/%Y"" id=""FIELD_GRYR"">2010</span> <span class=""jobdates"" format=""%m/%Y"" id=""FIELD_GRST""></span><span class=""hide"" dependency=""GRST+GRED""> – </span><span class=""jobdates"" format=""%m/%Y"" id=""FIELD_GRED""></span> <span class=""hide"" dependency=""GRED|GRST""><span class=""hide"" dependency=""GRIP"">–</span></span> <span id=""FIELD_GRIP""></span> </span> <span class=""paddedline degreeGap"" dependency=""DGRE|STUY|GRPA|GRHN""> <span class=""degree txt-bold txtItl"" id=""FIELD_DGRE"">Master of Science</span><span dependency=""DGRE+STUY""><span class=""beforecolonspace""> </span><span class=""txt-bold"">:</span></span> <span class=""programline"" id=""FIELD_STUY"">Electrical Engineering</span> <span dependency=""DGRE|STUY""><span class=""hide"" dependency=""GRHN""> - </span></span><span id=""FIELD_GRHN""></span> <span class=""xslt_static_change hide"" dependency=""GRPA"">GPA</span><span class=""hide"" dependency=""GRPA""><span class=""beforecolonspace""> </span><span>:</span> </span><span id=""FIELD_GRPA""></span> </span> <span id=""FIELD_FRFM""><div>GPA:3.62 </div><div>Courses: Computer Networks -Database Management Systems -Data and Computer Communications -Data Structures</div></span> </div> </div></div><div class=""paragraph PARAGRAPH_EDUC"" id=""PARAGRAPH_EDUC_8c4182fb-4323-4d0a-87e7-0cc334234265""><div class=""clearfix doc-item""> <div class=""singlecolumn""> <span class=""paddedline"" dependency=""SCHO""> <span class=""companyname txt-bold"" id=""FIELD_SCHO"">Jilin University</span> </span> <span class=""paddedline"" dependency=""SCIT|SSTA|GRYR|GRED|GRST|GRIP""> <span class=""joblocation jobcity"" id=""FIELD_SCIT"">Changchun</span><span dependency=""SCIT+SSTA"">, </span><span class=""joblocation jobstate"" id=""FIELD_SSTA"">Jilin</span> <span dependency=""GRYR|GRED|GRST|GRIP""><span class=""septr"" dependency=""SSTA|SCIT""></span></span> <span class=""xslt_static_change displayNoneThisField"">Expected in </span> <span format=""%m/%Y"" id=""FIELD_GRYR"">2007</span> <span class=""jobdates"" format=""%m/%Y"" id=""FIELD_GRST""></span><span class=""hide"" dependency=""GRST+GRED""> – </span><span class=""jobdates"" format=""%m/%Y"" id=""FIELD_GRED""></span> <span class=""hide"" dependency=""GRED|GRST""><span class=""hide"" dependency=""GRIP"">–</span></span> <span id=""FIELD_GRIP""></span> </span> <span class=""paddedline degreeGap"" dependency=""DGRE|STUY|GRPA|GRHN""> <span class=""degree txt-bold txtItl"" id=""FIELD_DGRE"">Bachelor of Science</span><span dependency=""DGRE+STUY""><span class=""beforecolonspace""> </span><span class=""txt-bold"">:</span></span> <span class=""programline"" id=""FIELD_STUY"">Telecommunications Engineering</span> <span dependency=""DGRE|STUY""><span class=""hide"" dependency=""GRHN""> - </span></span><span id=""FIELD_GRHN""></span> <span class=""xslt_static_change hide"" dependency=""GRPA"">GPA</span><span class=""hide"" dependency=""GRPA""><span class=""beforecolonspace""> </span><span>:</span> </span><span id=""FIELD_GRPA""></span> </span> <span id=""FIELD_FRFM"">GPA: 3.50</span> </div> </div></div></div></div><div class=""right-box"" id=""CONTAINER_3""><div class=""section section SECTION_SKLL firstsection"" data-section-cd=""SKLL"" id=""SECTION_SKLL44672d80-5dd3-4bb9-b8b8-8e33abf82e6f""><div class=""heading""><div class=""sectiontitle"" id=""SECTIONNAME_SKLL"">Skills</div></div><div class=""paragraph PARAGRAPH_SKLL firstparagraph"" id=""PARAGRAPH_SKLL_f7eea127-bf69-4874-bc0e-fd146d3cb3f6""><div class=""clearfix doc-item""> <div class=""field singlecolumn"" id=""FIELD_FRFM""><div><strong>
Roles</strong>:
Data Scientist, Data Analyst, Business System Analyst, Oracle PLSQL developer</div><div><strong>Data Visualization</strong>: D3.js, Tableau, R visualization packages, Microsoft Excel
</div><div><strong>Data Analytics Tools/Programming</strong>:
Python (numpy,
scipy, pandas, scikit-learn, gensim, keras), R(caret, weka, ggplot ), MATLAB,
Microsoft SQL Server, Oracle PLSQL

</div><div><strong>Machine Learning Algorithms</strong>: classification, regression, clustering, feature engineering
</div><div><strong>Big Data Tools</strong>: Hadoop, MapReduce, SQOOP, Pig, Hive, NOSQL, Cassandra, Spark
</div><div><strong>Others</strong>:Deep Learning, NLP, Topic Modeling, Sklearn, Graph Mining, Text Mining, C, C++ , Java, Javascript, ASP,
Shell Scripting 

Actively develop
predictive models and strategies for effective fraud detection for credit and

customer
banking activities using Kmeans clustering using Python.
</li><li>
Assisted senior data
scientist to do text mining on customer review/comment data, using topic
modeling

and sentimental
classification, using deep learning algorithms like CNN, RNN, LSTM, GRU, to

remediate according financial products using Python.

</li><li>Assisted
senior quantitative analyst in assessing risk management of financial
derivative products like foreign exchange products, bonds, funds, etc. using
machine learning
techniques for providing appropriate investment recommendations using
Collaborative filtering recommender
system using Python.
</li><li>
Mentored
sophisticated organizations on large scale customer data and analytics using
advanced

machine
learning and statistical models relying for issuing loan using Random Forest using R.

</li><li>Performed
k-Means clustering in order to understand customer backgrounds and segment the

customers
based on the customer transaction behavior information for customized product

offering,
customized and priority service, to improve existing profitable relationships
and to avoid customer churn, etc using R.
</li><li>
Worked on
Interactive Dashboards for building story and presenting to business using
Tableau. </li><li>Implementing Hadoop to provision big data analytics platforms for customer data. Used

MapReduce, Sqoop, Hive and Spark to migrate and analyze large call-quality-data datasets
from multiple Data sources like integrated funds transfer system like FedWire, CHIPS, SWIFT for  securities,
treasury or derivatives, and web-based cash management systems eGifts,
GiftsWEB, GiftsWEB EDD for fraud detection and risk management, for accounts
based on positive pay, and Automated Cash Handling, balance reporting, etc.
</li><li>
Installed and
configured Hadoop cluster in Test and Production environments; Moving data from
Oracle 9i database to HDFS and vice-versa using SQOOP; Collecting and
aggregating large amounts of log data using Apache Flume and staging data in
HDFS for further analysis; Developed multiple MapReduce jobs in java for data
cleaning and preprocessing; Writing Pig scripts to transform raw data from
several data sources into forming baseline data;  Solved performance
issues in Hive and Pig scripts with understanding of Joins, Group and aggregation
and how does it translate to MapReduce jobs; Developed Oozie workflow
for scheduling and coordinating the ETL process; Using Spark for further data
analysis/mining. </li><li>
Experience in using Sequence files, RCFile, AVRO and HAR file formats.</li><li>Work with
Data Analytics team to develop time series and optimization.
</li><li>
Involved in development and maintenance of
Oracle database using PLSQL and back-end

development
using C/C++ for intra-net management system for Employee Management System

(EMS)
and Agent Pay-out System (APS).

</li></ul></span> </div> </div></div><div ><div> <div > <span > <span >Accenture</span><span dependency=""COMP+JTIT""> - </span><span class=""jobtitle txt-bold"" id=""FIELD_JTIT"">Business System Analyst</span> <br dependency=""COMP|JTIT""/> <span class=""joblocation jobcity txtItl"" id=""FIELD_JCIT"">Overland Park</span><span dependency=""JCIT+JSTA|JCNT"">, </span><span class=""joblocation jobstate txtItl"" id=""FIELD_JSTA"">KS</span><span class=""hide"" dependency=""JSTA+JCNT"">, </span><span class=""joblocation jobcountry txtItl"" id=""FIELD_JCNT""></span><span class=""txtItl"" id=""FIELD_JLOC""></span> <span dependency=""JCIT|JSTA|JCNT|JLOC""><span class=""septr"" dependency=""JCTR|JSTD|EDDT""></span></span> <span class=""txtItl"" id=""FIELD_JCTR""></span> <span class=""hide"" dependency=""JCTR""><span class=""septr"" dependency=""JSTD|EDDT""></span></span> <span class=""jobdates txtItl"" format=""%m/%Y"" id=""FIELD_JSTD"">04/2009</span><span dependency=""JSTD+EDDT""> - </span><span class=""jobdates txtItl"" format=""%m/%Y"" id=""FIELD_EDDT"">08/2011</span> </span> <span class=""jobline"" id=""FIELD_JDES""><p><strong>Project Summary:</strong></p><p>This project is in
Application service group for Mercury system in Canon USA, Inc. , which mainly
in charge of the new item request, item disclosure between companies, item data
import from other Canon Americas companies to Canon USA., Canon Americas Master
inquiry, Model tree maintenance, model configurations, and camera/video
merchandise Maintenance for Canon Americas systems includes- S21 for Canon USA
merchandise master, S98 for Panama, CCI21 for Canada, S85 for Mexico, Chili,
Brazil, and Argentina, Ideal for Latin Americas countries.    </p><div>
<strong>Hardware/Software:</strong> 
Windows Vista/NT/XP/7, Linux, Oracle 11g/10g/9i/8i,
SQL*PLUS , Oracle SQL developer, Toad, 
Microsoft SQL Server management studio 2008,Microsoft Visual Studio
6.0,ODBC/JDBC , Microsoft IIS 5.1.1, Putty, Cygwin, Winmerge, VPN, ITG project
management system, PQedit, IIS, Autosys-PC-Xware 5.1.0, MS Word, Excel, Access,
Project, Visio   </div><div><strong>Responsibilities:</strong></div><ul><li>
Operational support for Canon Americas Mercury
system, includes data adjustment/research, batch data loading, system
migration, Technical and functional specification documentation, reports,
business process alignment, workflow stuck, and reconciliation, etc.    </li><li>Break/Fix any issues or bugs collected from
client and development regarding setup, performance, functionality, and
workflow stuck, etc. </li><li>System Enhancement regarding functionality and
performance, etc.</li><li>Reproduce and review existing oracle 9i schema
objects includes tables, temporary tables, views, materialized views, indexes,
triggers, procedures, functions, packages based on customer requirement and
system upgrade using Toad and Oracle SQL developer tools. 
</li><li>
Review and analyze ASP code for UMC Mercury
application web development for data research and system feature fix and
enhancement using Visual Interdev 6.0. 
</li><li>Query real-time data regarding Canon Americas
new item request, item status inquiry, item data disclosure and import, model
configuration and maintenance, warranty maintenance, and model configuration
inquiry , etc.  using complex SQL queries
on Oracle 9i Canon mercury database.    </li><li>Using Unix bash/corn shell scripting to do backend process operation, system resources checking, job scheduling, batch data loading, performance tuning and reporting.</li><li>Maintain scheduled day and night batch jobs for mercury system using Autosys-PC-Xware and Unix box and check the MQ series using PQ edit.  </li><li>
Implement client, session, action, module,
service, instance level end-to-end application tracing using SQL trace with
TKPROF, and Explain plan to check execution plans for high-load and Top SQL
statement.</li><li>Using Cygwin/ FTP- Putty with Unix Bash shell to
make a tunnel for Oracle database connection.
</li><li>Using Tortoise SVN for code check-out, update,
and release–comparison, etc.
</li><li>Tracking and documenting tickets for development
and reproduction Using ITG ticket tracking system.  
</li><li>
Assisted QA and build team to be involved
in unit, smoke, integration, system, UAT, non-functional testing regarding
performance, scalability, usability,  enduration, load, and volume testing, and  regression testing and maintenance using SOUP UI and Seapine QA Wizard Pro for product release.     
<br/></li><li>Data loading using Imp/exp, data pump, and
external tables from Americas Mercury system to S21-CUSA merchandise master, to
Ross-CUSA retails system, and Global Mercury system.
</li></ul></span> </div> </div></div><div class=""paragraph PARAGRAPH_EXPR"" id=""PARAGRAPH_EXPR_52bf1393-eff5-48bb-b183-eb65b32b5f5f""><div class=""clearfix doc-item""> <div class=""singlecolumn""> <span class=""paddedline"" dependency=""COMP|JCIT|JSTA|JCNT|JLOC|JTIT|JCTR|JSTD|EDDT""> <span class=""companyname txt-bold"" id=""FIELD_COMP"">Cognizant Technology Solutions</span><span dependency=""COMP+JTIT""> - </span><span class=""jobtitle txt-bold"" id=""FIELD_JTIT"">Oracle PLSQL Developer</span> <br dependency=""COMP|JTIT""/> <span class=""joblocation jobcity txtItl"" id=""FIELD_JCIT"">Novi</span><span dependency=""JCIT+JSTA|JCNT"">, </span><span class=""joblocation jobstate txtItl"" id=""FIELD_JSTA"">MI</span><span class=""hide"" dependency=""JSTA+JCNT"">, </span><span class=""joblocation jobcountry txtItl"" id=""FIELD_JCNT""></span><span class=""txtItl"" id=""FIELD_JLOC""></span> <span dependency=""JCIT|JSTA|JCNT|JLOC""><span class=""septr"" dependency=""JCTR|JSTD|EDDT""></span></span> <span class=""txtItl"" id=""FIELD_JCTR""></span> <span class=""hide"" dependency=""JCTR""><span class=""septr"" dependency=""JSTD|EDDT""></span></span> <span class=""jobdates txtItl"" format=""%m/%Y"" id=""FIELD_JSTD"">2008</span><span dependency=""JSTD+EDDT""> - </span><span class=""jobdates txtItl"" format=""%m/%Y"" id=""FIELD_EDDT"">04/2009</span> </span> <span class=""jobline"" id=""FIELD_JDES""><p><strong>Project Summary:</strong></p><p>NYU Langone Medical Center,
a world-class patient-centered integrated 
academic medical center, is one of the nation’s premier centers for
excellence in healthcare, biomedical research, and medical education. The project
is to develop new oracle database objects on online Health Information Managment system on FindWdev instance/server for 29 NYU medical school departments using in clinical,
education, research, and foundational areas, etc., to be used as Oracle staging
area to store the loaded data from NYU Medical Dash DWH from different source
systems, to provide further data to be
loaded into DWH for historical record, Decision support and Datamart for
reports,  and Cube for UI display.   </p><div>
<strong>Hardware/Software:</strong></div><div>
Unix,
Oracle 11g, Oracle developer 11g, Oracle EBS 11, ERP R12, IBM DataStage 8.0
(Designer, Director, Manager, Parallel Extender), Oracle Enterprise Manager, MS
SQL server management studio 2008, Toad for Oracle 9.0, TSQL &amp; PL/SQL, XML,
Erwin, Microsoft Visio, Autosys, IBM Data stage 8, Oracle reports 11g.</div><div>
<strong>Responsibilities: </strong></div><ul><li>
Independently develop Oracle
database objects includes tables, views, materialized views, indexes, triggers,
functions, procedures, packages, etc. </li><li>
Cooperated with BA, SME to
collect and document database design requirements and do data modeling with DB
architect using Erwin and Microsoft Visio. </li><li>
Assisted DBA for job
scheduling, data loading, and performance tuning using OEM, SQL tuning/access
advisor, hints,  explain plan , SQL trace
and V$ performance views under Unix. </li><li>
Write complicated queries
using a lot of aggregate functions, joins, analytical functions, subqueries ,
etc. to provide real-time data from
Oracle DB  for client and UI development
supporting.</li><li>Checking execution plan using
explain plan together with SQL trace with TKPROF using trcsess under unix to
realized end-to-end application tracing. 
</li><li>
Add optimization hints into
high–load and top SQL statements to change the optimization goal, access
method, join method, join order, and parallelization, etc.
</li><li>
Designed and developed ETL
processes using DataStage designer to load data from Oracle to staging database
and from staging to the target Data Warehouse.
</li><li>
Worked with Datastage Manager
for importing metadata from repository, new job Categories and creating new
data elements.</li><li>Used DataStage stages namely
Hash file, Sequential file, Transformer, Aggregate, Sort, Datasets, Join,
Lookup, Change Capture, Funnel, Peek, Row Generator stages in accomplishing the
ETL Coding.</li><li>Job scheduling using Autosys.
Coorporated with QA team for
debugging, unit, system, functional, UI, regression testing for new ISO release
production.
</li><li>
Working on Linux system for
batch data loading, job scheduling, and system resource checking, etc.  </li><li>Assisted back-end developer for reviewing and debugging C program for Health information management systems.</li><li>
Involved in
web development of online Health information management systems using JAVA.</li><li>Reviewed and
reproduced online JAVA reports of Health information management system, checked
DB references in it for intelligent Decision Support System
</li></ul></span> </div> </div></div></div></div></div></div>","Jessica Claire resumesample@example.com (555) 432-1000 , Montgomery Street , San Francisco , CA 94105 : Summary 7+ years of IT
Industry experience in in designing and developing data mining/analytics solutions,
data centric integration, developing and
maintaining Business analytics. Adept in data query, data migration, data analysis, predictive
modeling, machine learning, data mining, and data visualization implementations with extensive use of SQL, Python, R, Java, and Unix Shell scripting with platform of Toad, Oracle developer, Jupyter Notebook, Pycharm, R-Studio, Tableau, Hadoop with Spark. Experience in
predictive analytic procedures used in supervised learning (Classification,
Regression, Decision trees,
Random Forest, SVM, Neural Networks),
unsupervised learning (Clustering-k-Means and PCA), and reinforcement learning. Solid theory background for machine learning, data mining, text mining, graph mining, statistics modeling, NLP and deep learning. Expert in Natural
Language Processing like POS Tagging, Parsing, Named Entity Recognition,
Relationship extraction, Information       Retrieval, Sentimental
Classification, Machine Translation, etc. Solid knowledge of deep learning
algorithms like CNN, RNN, LSTM, GRU, etc. for text mining and image processing. Professional in writing complex SQL queries on Oracle, MS SQL server, Teradata and MySQL using a lot of subqueries, joins, aggregate functions, analytical functions, and temporary tables, etc.Worked on Big Data Analytics, Hadoop ecosystems (Hadoop, Sqoop, Hive, Pig, Mapreduce) and Spark for big data migration, cleaning, transformation, processing, query and analysis Familiar with software lifecycle includes requirement collecting/documentation, development, and testing for Unit, Smoke , integration, system, non-functional testing regarding performance,
scalability, usability, enduration,
load, and volume testing, and regression
testing, multivariate testing (A/B testing) and system maintenance. Work with business domain experts and application developers to identify data relevant analysis / mining and, develop new predictive / analytical modeling methods and/or tools in Financial (like loan and foreign exchange), Product, Customer, Sales domains, etc. Experience in data
aggregation and reduction techniques of large data sets with high performance
and parallel computing for high performance
analytical projects. Involved in
diagnosing and resolving predictive / analytical model performance issues,
monitoring analytical system performance·      
and implementing
efficiency improvements. Conversant with MS
SQL/ Oracle PLSQL and RDBMS. Contributed in data definitions for new database
file/table development and/or changes to
existing ones as needed for analysis and mining purpose. Experienced Oracle
PL/SQL Developer for designing, developing, debugging , maintaining and
administrating database in Oracle RDBMS. Solid
experience in PL/SQL and SQL programming and performance tuning. Familiarity with
Oracle data warehousing features such as star & snowflake data modeling
schemas, materialized views, bitmap indexes, Index
Organized Tables, external tables etc., and OLTP system using B-tree index,
Hash Join, etc. Experienced in
front-end developing using Java, Javascript, C++ and back-end developing using
C. Professional in
integrating and maintaining code using version control tools PVCS, SVN,
CVS. Solid experience and
knowledge in ETL and Data warehousing concepts.Data Processing experience in
designing, implementing transformation
processes using ETL tool. Involved in all aspects of ETL- requirement gathering
with standard interfaces to be used by operational
sources, data cleaning, data loading strategies, ETL mappings designing,
documentation and ETL jobs performance testing. Using Unix
bash/corn shell scripting to do backend process operation, system resources
checking, job
scheduling, batch data loading, performance tuning and reporting. Conversant with
Project Management deliverables and SDLC phases – Waterfall and Agile. A self-starter, team
player, excellent communicator, prolific researcher. Expert technical
documentation skills. Strong interpersonal and communication skills (both
written and oral); ability to communicate with
people in a wide variety of areas and at various levels from technical
specialists to senior management. Education University of California Los Angeles , CA Expected in 2016 – – Master of Science : Computer Science - GPA : GPA:3.78 Courses:
-Statistics Programming -Databases and
Knowledge Bases -Graphs and Network Flows
-Language and Thought -Current Topics in
Computer Theory(Machine Learning Algorithm) -Computer Science Classics(Basic
Data Science) -Data Mining and
Big Data Analytics -System Security University of Bridgeport Bridgeport , CT Expected in 2010 – – Master of Science : Electrical Engineering - GPA : GPA:3.62 Courses: Computer Networks -Database Management Systems -Data and Computer Communications -Data Structures Jilin University Changchun , Jilin Expected in 2007 – – Bachelor of Science : Telecommunications Engineering - GPA : GPA: 3.50 Skills Roles :
Data Scientist, Data Analyst, Business System Analyst, Oracle PLSQL developer Data Visualization : D3.js, Tableau, R visualization packages, Microsoft Excel Data Analytics Tools/Programming :
Python (numpy,
scipy, pandas, scikit-learn, gensim, keras), R(caret, weka, ggplot ), MATLAB,
Microsoft SQL Server, Oracle PLSQL Machine Learning Algorithms : classification, regression, clustering, feature engineering Big Data Tools : Hadoop, MapReduce, SQOOP, Pig, Hive, NOSQL, Cassandra, Spark Others :Deep Learning, NLP, Topic Modeling, Sklearn, Graph Mining, Text Mining, C, C++ , Java, Javascript, ASP,
Shell Scripting Experience Lockheed Martin Corporation - Data Scientist Yuma , AZ , 08/2011 - 09/2016 Actively develop
predictive models and strategies for effective fraud detection for credit and
customer
banking activities using Kmeans clustering using Python. Assisted senior data
scientist to do text mining on customer review/comment data, using topic
modeling
and sentimental
classification, using deep learning algorithms like CNN, RNN, LSTM, GRU, to
remediate according financial products using Python. Assisted
senior quantitative analyst in assessing risk management of financial
derivative products like foreign exchange products, bonds, funds, etc. using
machine learning
techniques for providing appropriate investment recommendations using
Collaborative filtering recommender
system using Python. Mentored
sophisticated organizations on large scale customer data and analytics using
advanced

machine
learning and statistical models relying for issuing loan using Random Forest using R. Performed
k-Means clustering in order to understand customer backgrounds and segment the

customers
based on the customer transaction behavior information for customized product

offering,
customized and priority service, to improve existing profitable relationships
and to avoid customer churn, etc using R. Worked on
Interactive Dashboards for building story and presenting to business using
Tableau. Implementing
Hadoop to provision big data analytics platforms for customer data. Used

MapReduce,
Sqoop, Hive and Spark to migrate and analyze large call-quality-data datasets
from

multiple Data
sources like integrated funds transfer system like FedWire, CHIPS, SWIFT for

securities,
treasury or derivatives, and web-based cash management systems eGifts,
GiftsWEB, GiftsWEB EDD for fraud detection and risk management, for accounts
based on positive pay, and Automated Cash Handling, balance reporting, etc. Installed and
configured Hadoop cluster in Test and Production environments; Moving data from
Oracle 9i database to HDFS and vice-versa using SQOOP; Collecting and
aggregating large amounts of log data using Apache Flume and staging data in
HDFS for further analysis; Developed multiple MapReduce jobs in java for data
cleaning and preprocessing; Writing Pig scripts to transform raw data from
several data sources into forming baseline data;  Solved performance
issues in Hive and Pig scripts with understanding of Joins, Group and aggregation
and how does it translate to MapReduce jobs; Developed Oozie workflow
for scheduling and coordinating the ETL process; Using Spark for further data
analysis/mining. Experience in
using Sequence files, RCFile, AVRO and HAR file formats. Work with
Data Analytics team to develop time series and optimization. Involved in development and maintenance of
Oracle database using PLSQL and back-end

development
using C/C++ for intra-net management system for Employee Management System

(EMS)
and Agent Pay-out System (APS). Accenture - Business System Analyst Overland Park , KS , 04/2009 - 08/2011 Project Summary: This project is in
Application service group for Mercury system in Canon USA, Inc. , which mainly
in charge of the new item request, item disclosure between companies, item data
import from other Canon Americas companies to Canon USA., Canon Americas Master
inquiry, Model tree maintenance, model configurations, and camera/video
merchandise Maintenance for Canon Americas systems includes- S21 for Canon USA
merchandise master, S98 for Panama, CCI21 for Canada, S85 for Mexico, Chili,
Brazil, and Argentina, Ideal for Latin Americas countries. Hardware/Software: Windows Vista/NT/XP/7, Linux, Oracle 11g/10g/9i/8i,
SQL*PLUS , Oracle SQL developer, Toad, 
Microsoft SQL Server management studio 2008,Microsoft Visual Studio
6.0,ODBC/JDBC , Microsoft IIS 5.1.1, Putty, Cygwin, Winmerge, VPN, ITG project
management system, PQedit, IIS, Autosys-PC-Xware 5.1.0, MS Word, Excel, Access,
Project, Visio Responsibilities: Operational support for Canon Americas Mercury
system, includes data adjustment/research, batch data loading, system
migration, Technical and functional specification documentation, reports,
business process alignment, workflow stuck, and reconciliation, etc. Break/Fix any issues or bugs collected from
client and development regarding setup, performance, functionality, and
workflow stuck, etc. System Enhancement regarding functionality and
performance, etc. Reproduce and review existing oracle 9i schema
objects includes tables, temporary tables, views, materialized views, indexes,
triggers, procedures, functions, packages based on customer requirement and
system upgrade using Toad and Oracle SQL developer tools. Review and analyze ASP code for UMC Mercury
application web development for data research and system feature fix and
enhancement using Visual Interdev 6.0. Query real-time data regarding Canon Americas
new item request, item status inquiry, item data disclosure and import, model
configuration and maintenance, warranty maintenance, and model configuration
inquiry , etc.  using complex SQL queries
on Oracle 9i Canon mercury database. Using Unix bash/corn shell scripting to do backend process operation, system resources checking, job scheduling, batch data loading, performance tuning and reporting. Maintain scheduled day and night batch jobs for mercury system using Autosys-PC-Xware and Unix box and check the MQ series using PQ edit. Implement client, session, action, module,
service, instance level end-to-end application tracing using SQL trace with
TKPROF, and Explain plan to check execution plans for high-load and Top SQL
statement. Using Cygwin/ FTP- Putty with Unix Bash shell to
make a tunnel for Oracle database connection. Using Tortoise SVN for code check-out, update,
and release–comparison, etc. Tracking and documenting tickets for development
and reproduction Using ITG ticket tracking system. Assisted QA and build team to be involved
in unit, smoke, integration, system, UAT, non-functional testing regarding
performance, scalability, usability,  enduration, load, and volume testing, and  regression testing and maintenance using SOUP UI and Seapine QA Wizard Pro for product release. Data loading using Imp/exp, data pump, and
external tables from Americas Mercury system to S21-CUSA merchandise master, to
Ross-CUSA retails system, and Global Mercury system. Cognizant Technology Solutions - Oracle PLSQL Developer Novi , MI , 2008 - 04/2009 Project Summary: NYU Langone Medical Center,
a world-class patient-centered integrated 
academic medical center, is one of the nation’s premier centers for
excellence in healthcare, biomedical research, and medical education. The project
is to develop new oracle database objects on online Health Information Managment system on FindWdev instance/server for 29 NYU medical school departments using in clinical,
education, research, and foundational areas, etc., to be used as Oracle staging
area to store the loaded data from NYU Medical Dash DWH from different source
systems, to provide further data to be
loaded into DWH for historical record, Decision support and Datamart for
reports,  and Cube for UI display. Hardware/Software: Unix,
Oracle 11g, Oracle developer 11g, Oracle EBS 11, ERP R12, IBM DataStage 8.0
(Designer, Director, Manager, Parallel Extender), Oracle Enterprise Manager, MS
SQL server management studio 2008, Toad for Oracle 9.0, TSQL & PL/SQL, XML,
Erwin, Microsoft Visio, Autosys, IBM Data stage 8, Oracle reports 11g. Responsibilities: Independently develop Oracle
database objects includes tables, views, materialized views, indexes, triggers,
functions, procedures, packages, etc. Cooperated with BA, SME to
collect and document database design requirements and do data modeling with DB
architect using Erwin and Microsoft Visio. Assisted DBA for job
scheduling, data loading, and performance tuning using OEM, SQL tuning/access
advisor, hints,  explain plan , SQL trace
and V$ performance views under Unix. Write complicated queries
using a lot of aggregate functions, joins, analytical functions, subqueries ,
etc. to provide real-time data from
Oracle DB  for client and UI development
supporting. Checking execution plan using
explain plan together with SQL trace with TKPROF using trcsess under unix to
realized end-to-end application tracing. Add optimization hints into
high–load and top SQL statements to change the optimization goal, access
method, join method, join order, and parallelization, etc. Designed and developed ETL
processes using DataStage designer to load data from Oracle to staging database
and from staging to the target Data Warehouse. Worked with Datastage Manager
for importing metadata from repository, new job Categories and creating new
data elements. Used DataStage stages namely
Hash file, Sequential file, Transformer, Aggregate, Sort, Datasets, Join,
Lookup, Change Capture, Funnel, Peek, Row Generator stages in accomplishing the
ETL Coding. Job scheduling using Autosys.

Coorporated with QA team for
debugging, unit, system, functional, UI, regression testing for new ISO release
production. Working on Linux system for
batch data loading, job scheduling, and system resource checking, etc. Assisted back-end developer for reviewing and debugging C program for Health information management systems. Involved in
web development of online Health information management systems using JAVA. Reviewed and
reproduced online JAVA reports of Health information management system, checked
DB references in it for intelligent Decision Support System"
